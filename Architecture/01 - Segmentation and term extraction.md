# Модель для выделения ключевых слов и выражений из загруженного текста

## 1. Архитектура системы и потоки данных

### 1.1. Общая схема взаимодействия модулей

```
┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│   Загрузка    │     │  Сегментация  │     │Предварительная│
│   документа   ├────►│    текста     ├────►│    оценка     │
└───────┬───────┘     └───────┬───────┘     └───────┬───────┘
        │                     │                     │
        │                     │                     │
        │                     │                     ▼
┌───────▼───────┐     ┌───────▼───────┐     ┌───────────────┐
│   Индексация  │     │ Постобработка │     │  Обработка    │
│   контекста   │◄────┤  и связывание │◄────┤    текста     │
└───────┬───────┘     └───────┬───────┘     └───────┬───────┘
        │                     │                     │
        │                     │                     │
        ▼                     ▼                     ▼
┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│Визуализация и │     │  Оценка       │     │  Выделение    │
│    вывод      │◄────┤  качества     │◄────┤ ключевых слов │
└───────────────┘     └───────────────┘     └───────────────┘
```

### 1.2. Интерфейсы взаимодействия между модулями

- **Выходные интерфейсы модуля**:
    - `SegmentedDocumentAPI`: Предоставляет доступ к сегментированному документу с метаданными
        - Методы: `getSegments()`, `getSegmentById(id)`, `getDocumentMetadata()`
        - Формат данных: JSON-структура с полями для текста, идентификаторов и позиционных метаданных
    - `ExtractedTerminologyAPI`: Предоставляет доступ к извлеченным терминам и их контекстам
        - Методы: `getTerms()`, `getTermContexts(termId)`, `getTermMetadata(termId)`
        - Формат данных: Структурированный словарь терминов с их атрибутами и связями
- **Входные зависимости**:
    - `DocumentProcessingAPI`: Интерфейс для получения исходных документов
        - Требуемые методы: `getDocumentContent()`, `getDocumentMetadata()`
    - `ConfigurationAPI`: Интерфейс для получения конфигурационных параметров
        - Требуемые методы: `getSegmentationRules()`, `getTermExtractionParameters()`
- **Механизмы синхронизации**:
    - События завершения обработки: `onSegmentationComplete`, `onTermExtractionComplete`
    - Механизм уведомлений для асинхронных взаимодействий через шину сообщений
    - Контрольные точки для восстановления при сбоях

## 2. Этапы обработки

### 2.1. Загрузка документа

- Поддержка разнообразных форматов (PDF, DOCX, XLSX, HTML, TXT, RTF и т.д.)
- Извлечение текста с учётом структуры документа
- Парсинг таблиц и структурированных данных с использованием специализированных инструментов:
    - Tabula/Camelot для таблиц в PDF
    - Pandas для таблиц в XLSX/CSV
    - BeautifulSoup для структурированных HTML-таблиц
- Масштабируемая обработка документов объёмом более тысячи страниц
- Сохранение метаданных документа для дальнейшей обработки

### 2.2. Текстовая сегментация

- Объединение разорванных строк в целостные блоки текста
- Продвинутое сегментирование текста на предложения с учётом:
    - Сокращений (т.е., др., проф., и т.д., etc.)
    - Технических обозначений (например, 3.14, IP-адресов, версий ПО)
    - Многострочных предложений, разделённых переносами строк
    - Цитат и вложенных конструкций
- Сохранение контекстной целостности предложений
- Восстановление предложений, разбитых переносами строк
- Размер "окна" контекста определяется как:
    - Стандартно: ±2 предложения от целевого
    - Адаптивно: расширяется для сохранения смысловой целостности абзаца
    - Настраивается в параметрах конфигурации
- Использование ML-моделей для улучшения качества сегментации
- Параллельная обработка для больших документов
- Генерация уникальных идентификаторов для каждого сегмента

### 2.3. Предварительная оценка документа

- Определение объёма и сложности текста на основе сегментированного контента
- Анализ структуры документа (заголовки, разделы, списки)
- Предварительная классификация типа контента и тематики:
    - Использование иерархических классификаторов (например, 1-й уровень: медицина/право/техника, 2-й уровень: подобласти)
    - Процентная вероятность принадлежности к каждой категории
- Выбор оптимальной стратегии обработки
- Определение предметной области для выбора доменных моделей:
    - Автоматический выбор наиболее подходящих доменных моделей на основе классификации
    - Настраиваемые пороги уверенности для использования доменных моделей

### 2.4. Обработка текста

- Определение языка текста с использованием ансамбля детекторов:
    - langdetect для быстрого предварительного определения
    - fastText для повышения точности на коротких текстах
    - Взвешивание результатов с учётом уверенности моделей
- Очистка (удаление HTML-тегов, символов, нормализация)
- Токенизация с учётом особенностей сегментированного текста
- Удаление стоп-слов с учётом языка и предметной области
- Лемматизация и стемминг
- Применение специализированных доменных моделей для обработки

### 2.5. Извлечение ключевых слов и выражений

- Статистические методы (TF-IDF, BM25)
- Графовые методы (TextRank, PositionRank)
- Модели машинного обучения (BERT, KeyBERT, YAKE!)
- Контекстные эмбеддинги (Sentence-BERT) для кластеризации похожих терминов
- Использование специализированных словарей и доменных NER-моделей:
    - Адаптация через дообучение на доменных корпусах
    - Использование специализированных словарей для терминов
    - Доменные правила для выделения сложных сущностей
- Выделение многословных выражений и коллокаций:
    - KeyphraseVectorizers для выделения фраз из нескольких слов
    - Статистические методы (PMI, Log-Likelihood) для анализа коллокаций
    - Использование специализированных моделей для терминологической экстракции
- Применение предобученных доменных моделей (SciSpacy, Med7, Legal-BERT и др.)
- Учет сегментированных предложений для контекстного анализа
- Сохранение связи «ключевое слово → сегмент» для каждого найденного термина
- Настраиваемые пороги для минимальной значимости ключевых слов (по умолчанию: 0.3)

### 2.6. Постобработка

- Фильтрация нерелевантных ключевых слов с настраиваемыми порогами
- Кластеризация и группировка семантически схожих ключевых выражений:
    - Настраиваемый порог кластеризации (по умолчанию: 0.75 для HDBSCAN)
    - Определение представителя кластера через центроид или наиболее частотный термин
- Приведение к единой форме (лемматизация, нормализация)
- Ранжирование по значимости с учётом структуры документа
- Семантическое ранжирование ключевых слов на основе контекстных эмбеддингов:
    - Использование SentenceBERT для выявления семантически значимых терминов
    - Анализ расположения термина в тексте (заголовок, начало раздела и т.д.)
    - Учёт частотности и глобальной значимости терминов
- Объединение результатов разных методов экстракции
- Применение доменных словарей для проверки и нормализации терминов
- Индексация контекстов употребления ключевых слов

### 2.7. Индексация ключевых слов и их контекстов

- Построение двусторонних индексов (ключевое слово → сегменты, сегмент → ключевые слова)
- Ранжирование сегментов по релевантности для каждого ключевого слова
- Выявление связей между ключевыми словами на основе совместной встречаемости
- Кластеризация сегментов для определения разных контекстов употребления
- Создание легковесных структур индексации для быстрого поиска
- Инкрементальное обновление индексов:
    - Отслеживание изменений документа через хеш-суммы
    - Обновление только изменённых частей индекса
    - Поддержка версионирования индексов для истории изменений
- Стратегии обновления: автоматически при изменении документа или по запросу пользователя

### 2.8. Выходные данные

- Формирование структурированного отчета с ключевыми словами и их значимостью
- Контекстуализация ключевых слов с сохранением сегментов их употребления
- Построение индекса связей между ключевыми словами и сегментами текста
- Визуализация (облако тегов, граф связей, тематические кластеры)
- Экспорт в различных форматах (JSON, CSV, HTML, PDF) с контекстами употребления
- Аннотированная версия исходного документа с выделенными ключевыми словами
- Интерфейс для навигации между ключевыми словами и их контекстами

### 2.9. Оценка качества

- Метрики для оценки релевантности выделенных ключевых слов
- Сравнение с эталонными наборами
- Система обратной связи для улучшения модели
- Мониторинг производительности разных методов
- Доменно-специфичная оценка качества выделения терминов
- Оценка качества сегментации текста

## 3. Модули проекта

### 3.1. Модуль аутентификации и управления доступом

- Аутентификация пользователей с использованием различных провайдеров
- Ролевая модель для разграничения доступа к функциональности
- Аудит действий пользователей
- Управление API-ключами для автоматизированного доступа
- Интеграция с корпоративными системами единого входа (SSO)

### 3.2. Модуль загрузки документов

- Универсальный обработчик на базе Apache Tika
- Специализированные обработчики файлов (pdf, docx, xlsx, html и т.д.)
- Обработка многостраничных документов
- Поддержка различных кодировок
- Масштабируемая параллельная обработка с использованием Dask/Ray

### 3.3. Модуль сегментации текста

- Алгоритмы обнаружения и объединения разорванных предложений
- Обработка переносов строк с учётом контекста
- Распознавание сокращений и специальных конструкций (т.е., т.д., etc.)
- Использование регулярных выражений для обработки сложных случаев
- Применение моделей глубокого обучения для сегментации сложных текстов
- Словари сокращений и аббревиатур
- Параллельная сегментация с использованием Dask/Ray для повышения производительности
- Генерация уникальных идентификаторов для каждого сегмента

### 3.4. Модуль языкового определения и анализа

- Комбинированное использование FastText и langdetect для определения языка
- Выбор подходящих NLP-моделей для обработки на основе языка
- Анализ структуры документа и выделение важных секций
- Детектирование предметной области для выбора специализированных моделей

### 3.5. Модуль определения предметной области

- Классификаторы документов по предметным областям
- Иерархическая структура категорий
- Многометочная классификация для документов на стыке областей
- Логика выбора доменных моделей на основе классификации
- Настраиваемые пороги уверенности для использования специализированных моделей

### 3.6. Модуль обработки текста

- Очистка (удаление HTML-тегов, символов, нормализация)
- Токенизация и сегментация на предложения
- Лемматизация с учётом особенностей языка (Spacy, Stanza)
- Морфологический и синтаксический анализ
- Распознавание именованных сущностей (NER)
- Параллельная обработка с использованием Polars/Dask

### 3.7. Модуль конфигурации и настройки параметров

- Управление весами различных алгоритмов
- Настройка параметров для разных языков
- Конфигурация порогов для фильтрации результатов:
    - Минимальная значимость ключевых слов
    - Пороги кластеризации для группировки терминов
    - Настройки окна контекста
- Адаптация под различные предметные области
- Выбор и настройка специализированных доменных моделей
- Конфигурация правил сегментации для различных типов текстов
- Настройки индексации контекстов ключевых слов

### 3.8. Модуль доменных моделей

- Библиотека предобученных доменных моделей
- Медицинские модели (SciSpacy, Med7, BioBERT)
- Юридические модели (Legal-BERT, юридические словари)
- Технические модели (модели для ИТ, инженерии, патентов)
- Финансовые модели (FinBERT, словари финансовых терминов)
- Научные модели (SciBERT, специализированные модели по отраслям науки)
- Механизм выбора и переключения между моделями
- Доменные словари сокращений и специальных терминов

### 3.9. Модуль выделения ключевых слов

- Мультистратегический подход (TF-IDF, TextRank, KeyBERT, YAKE!)
- Доменные NER-модели (SpaCy, Hugging Face)
- Работа с многословными выражениями (KeyphraseVectorizers)
- Использование контекстных эмбеддингов для уточнения терминов
- Распределенная обработка больших документов с Ray/Dask
- Интеграция с доменными моделями для специализированной экстракции
- Поддержка выделения терминов на уровне сегментированных предложений
- Отслеживание позиций ключевых слов в сегментах

### 3.10. Модуль семантического ранжирования

- Вычисление эмбеддингов ключевых слов и их контекстов
- Оценка семантической значимости терминов
- Учёт структурного положения термина в документе
- Анализ взаимосвязей между терминами
- Использование SentenceBERT/GloVe для построения векторных представлений
- Интеграция с модулем индексации для повышения точности поиска

### 3.11. Модуль индексации контекста ключевых слов

- Построение двусторонних индексов (ключевые слова → сегменты, сегменты → ключевые слова)
- Ранжирование сегментов по их релевантности для каждого ключевого слова
- Кластеризация контекстов для выявления различных употреблений терминов
- Выявление связей между ключевыми словами на основе совместной встречаемости
- Оптимизация структур данных для эффективного поиска и доступа к контекстам
- Поддержка инкрементальных обновлений при изменении документа
- Сжатие и оптимизация индексов для экономии памяти
- Параллельная индексация для больших документов

### 3.12. Модуль постобработки

- Фильтрация и ранжирование ключевых слов
- Кластеризация с использованием HDBSCAN и эмбеддингов SentenceBERT
- Группировка и нормализация полученных терминов
- Объединение результатов разных методов извлечения
- Параллельная постобработка больших наборов ключевых слов
- Обогащение ключевых слов контекстной информацией из сегментов
- Нормализация контекстов для улучшения качества поиска

### 3.13. Модуль кэширования

- Сохранение промежуточных результатов для больших документов
- Переиспользование результатов для схожих документов
- Асинхронная обработка и сохранение состояний
- Возможность возобновления прерванной обработки
- Распределенное кэширование с использованием Redis/Memcached
- Кэширование контекстов часто используемых ключевых слов

### 3.14. Модуль масштабирования

- Параллельная обработка с использованием Dask
- Распределенные вычисления с Ray
- Очереди задач с использованием Celery или Ray Actors
- Оптимизированная обработка данных с Polars
- Горизонтальное масштабирование компонентов:
    - Независимое масштабирование модулей сегментации, обработки и индексации
    - Микросервисная архитектура для изолированной работы компонентов
- Разделение больших документов на блоки для параллельной обработки
- Балансировка нагрузки при обработке сверхбольших документов
- Управление памятью и ресурсами для предотвращения перегрузок
- Масштабируемая индексация контекстов для больших корпусов документов

### 3.15. Модуль оценки качества

- Метрики для оценки релевантности выделенных ключевых слов
- Сравнение с эталонными наборами
- Система обратной связи для улучшения модели
- Мониторинг производительности разных методов
- Доменно-специфичная оценка качества выделения терминов
- Оценка качества сегментации текста
- Анализ полноты и качества индексирования контекстов

### 3.16. Модуль визуализации и вывода

- Генерация структурированных отчетов (JSON/CSV/HTML) с контекстами
- Построение облака тегов / графа связей / тематических кластеров
- Интерактивные визуализации для анализа результатов
- Экспорт данных в различных форматах
- Аннотирование исходного документа
- Оптимизированная визуализация для больших наборов ключевых слов
- Визуализация связей ключевых слов с сегментированными предложениями
- Интерфейс для навигации между ключевыми словами и их контекстами

### 3.17. Модуль контекстного поиска

- Поиск по ключевым словам с возвратом релевантных сегментов
- Фасетный поиск по комбинации ключевых слов
- Ранжирование результатов поиска по релевантности
- Семантический поиск по смысловой близости
- Интерфейс для навигации между связанными ключевыми словами
- Выделение ключевых слов в найденных сегментах
- Поддержка расширенного синтаксиса поисковых запросов

## 4. Технологический стек

### 4.1. Инструменты для обработки документов

- Apache Tika - универсальный анализатор контента
- pdfplumber/pypdf2 - для работы с PDF
- Tabula/Camelot - для извлечения таблиц из PDF
- python-docx - для работы с DOCX
- openpyxl - для работы с XLSX
- BeautifulSoup - для обработки HTML

### 4.2. NLP библиотеки и модели

- Spacy - для основной обработки текста и продвинутой сегментации
- Stanza - альтернативный инструмент для языков с ограниченной поддержкой в Spacy
- NLTK - для базовых операций и статистических методов
- Transformers (Hugging Face) - для современных NLP моделей
- fastText и langdetect в комбинации - для более точного определения языка
- spaCy-sentence-segmenter - для улучшенной сегментации предложений
- DeepSegment - для сегментации с помощью глубокого обучения

### 4.3. Доменные модели и инструменты

- SciSpacy - для научных и медицинских текстов
- Med7 - для медицинской информации
- Legal-BERT - для юридических документов
- BioBERT - для биомедицинских текстов
- FinBERT - для финансовых документов
- SciBERT - для научных текстов
- Предметно-ориентированные словари и онтологии
- Словари сокращений и аббревиатур по отраслям

### 4.4. Модели для выделения ключевых слов

- KeyBERT - для извлечения ключевых слов с помощью BERT
- YAKE! - для извлечения многословных выражений
- pke (Python Keyphrase Extraction) - для различных алгоритмов
- SentenceTransformers - для работы с эмбеддингами
- KeyphraseVectorizers - для выделения многословных выражений
- GloVe - для семантического ранжирования ключевых слов

### 4.5. Инструменты для кластеризации и постобработки

- scikit-learn - для классических алгоритмов ML
- HDBSCAN - для кластеризации
- SentenceBERT - для семантической близости
- networkx - для графовых алгоритмов

### 4.6. Инструменты для индексации и поиска

- Elasticsearch - для полнотекстового поиска и индексации контекстов
- PyLucene - для локальной индексации и поиска
- FAISS - для векторного поиска семантически похожих контекстов
- SQLite/PostgreSQL - для хранения связей ключевых слов и сегментов
- Redis - для кэширования часто используемых индексов
- Pyserini - для быстрого поиска в индексированных сегментах
- whoosh - легковесная библиотека для полнотекстового поиска

### 4.7. Инструменты для масштабирования

- Dask - для параллельной и распределенной обработки
- Ray - для распределенных вычислений и ML
- Celery - для очередей задач и асинхронной обработки
- Polars - для быстрой обработки данных
- Redis/Memcached - для распределенного кэширования
- joblib - для параллелизма на уровне функций

### 4.8. Инструменты для визуализации и вывода

- matplotlib - для базовых визуализаций
- plotly - для интерактивных визуализаций
- wordcloud - для облаков тегов
- pyLDAvis - для визуализации тематических моделей
- Dash - для интерактивных дашбордов
- NetworkX/Cytoscape - для визуализации связей между ключевыми словами
- Vue.js/React (фронтенд) - для интерактивных интерфейсов навигации по контекстам

### 4.9. Инструменты для аутентификации и безопасности

- OAuth 2.0 / OpenID Connect - для федеративной аутентификации
- JWT - для безопасной передачи токенов
- RBAC (Role-Based Access Control) - для управления доступом
- Redis/Memcached - для хранения сессий

## 5. Конфигурируемые параметры

### 5.1. Основные пороговые значения

- **min_keyword_score**: 0.3 - минимальная значимость ключевого слова
- **clustering_threshold**: 0.75 - порог для кластеризации похожих терминов
- **domain_confidence_threshold**: 0.6 - минимальная уверенность для выбора доменной модели
- **context_window_size**: 2 - размер окна контекста (количество предложений до/после)
- **max_keywords_per_segment**: 10 - максимальное количество ключевых слов на сегмент
- **min_segment_length**: 5 - минимальная длина сегмента в токенах для обработки
- **relevance_threshold**: 0.4 - минимальная релевантность сегмента для ключевого слова

### 5.2. Веса методов выделения ключевых слов

- **tfidf_weight**: 0.3
- **textrank_weight**: 0.2
- **keybert_weight**: 0.3
- **yake_weight**: 0.2

### 5.3. Настройки масштабирования

- **parallel_segments**: 100 - количество сегментов для параллельной обработки
- **batch_size**: 1000 - размер пакета для обработки больших документов
- **max_workers**: 8 - максимальное количество параллельных рабочих процессов

### 5.4. Настройки кэширования

- **cache_ttl**: 3600 - время жизни кэша в секундах
- **cache_size_limit**: 1024 - максимальный размер кэша в МБ

## 7. Обработка особых случаев

### 7.1. Инкрементальные обновления документов

- Отслеживание изменений в исходном документе через хеш-суммы или версионирование
- Повторная обработка только изменённых сегментов
- Сохранение истории изменений ключевых слов для анализа динамики
- Стратегии обновления: автоматически или по запросу пользователя

### 7.2. Многоязычные фрагменты

- Определение языка на уровне сегментов
- Применение соответствующих языковых моделей для каждого сегмента
- Объединение результатов с учётом особенностей каждого языка

### 7.3. Специфические доменные конструкции

- Распознавание формул, кодов, специфических обозначений
- Сохранение их как единых сущностей без разбиения на отдельные токены
- Применение доменно-специфичных моделей для распознавания терминологии

### 7.4. Сверхбольшие документы

- Разделение на логические блоки для параллельной обработки
- Распределённое хранение индексов
- Асинхронная обработка с возможностью получения промежуточных результатов
- Стратегии оптимизации памяти для индексации и поиска

## 8. Мониторинг и оценка производительности

### 8.1. Метрики оценки качества выделения ключевых слов

- Precision, Recall, F1-score по отношению к эталонным наборам
- Метрики на основе экспертной оценки (Precision@K, MAP)
- Оценка полноты охвата документа выделенными ключевыми словами
- Внутрисегментные и межсегментные связи ключевых слов

### 8.2. Мониторинг системы

- Время обработки документа в зависимости от его размера
- Использование ресурсов (CPU, память, диск)
- Точность определения языка и предметной области
- Качество сегментации текста
- Эффективность кэширования и переиспользования результатов

## 9. Интеграция с внешними системами

### 9.1. API для внешних систем

- REST API для получения ключевых слов из документа
- Возможность интеграции с системами документооборота
- Webhooks для уведомления о завершении обработки
- GraphQL API для сложных запросов к индексированным ключевым словам

### 9.2. Поддержка облачных хранилищ

- Интеграция с S3, Google Drive, Dropbox и т.д.
- Обработка документов напрямую из облачных хранилищ
- Сохранение результатов в облачные хранилища

### 9.3. Интеграция с аналитическими платформами

- Экспорт результатов в форматах, совместимых с BI-системами
- Интеграция с системами полнотекстового поиска (Elasticsearch, Solr)
- Поддержка потоковой обработки для встраивания в аналитические пайплайны`
    - Метаданные содержат: тип документа, количество страниц, структуру (заголовки, разделы)

- **Загрузка документа** → **Сегментация**:
    - Формат: Структурированный словарь `{text: str, metadata: dict}

- **Сегментация** → **Предварительная оценка**:
    
    - Формат: Список сегментов `[{id: str, text: str, position: dict}]`
    - Позиция содержит: номер страницы, абзац, порядковый номер предложения
- **Предварительная оценка** → **Обработка текста**:
    
    - Формат: Обогащённый список сегментов с метаданными `[{id: str, text: str, domain: str, complexity: float}]`
    - Предметная область и сложность для каждого сегмента
- **Обработка текста** → **Выделение ключевых слов**:
    
    - Формат: Обработанные сегменты `[{id: str, text: str, tokens: list, lemmas: list, pos_tags: list}]`
    - Токенизированный и лемматизированный текст с частями речи
- **Выделение ключевых слов** → **Постобработка**:
    
    - Формат: Ключевые слова по сегментам `{keywords: [{text: str, score: float, method: str}], segment_keywords: dict}`
- **Постобработка** → **Индексация контекста**:
    
    - Формат: Нормализованные ключевые слова с сегментами `{normalized_keywords: dict, segment_mapping: dict}`
- **Индексация контекста** → **Визуализация**:
    
    - Формат: Индексированные данные по ключевым словам и сегментам, готовые для поиска и отображения

