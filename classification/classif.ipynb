{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты классификации для файла: Masterpact.pdf\n",
      "================================================================================\n",
      "1. Электрические автоматы и выключатели\n",
      "   Уверенность: 55.90%\n",
      "   Количество совпадений: 161\n",
      "   Ключевые термины:\n",
      "     - masterpact: 70\n",
      "     - выключатель: 57\n",
      "     - расцепитель: 17\n",
      "     - отключение: 12\n",
      "     - автоматический выключатель: 1\n",
      "----------------------------------------\n",
      "2. Электропитание и распределение\n",
      "   Уверенность: 15.62%\n",
      "   Количество совпадений: 45\n",
      "   Ключевые термины:\n",
      "     - напряжение: 13\n",
      "     - ток: 13\n",
      "     - питание: 10\n",
      "     - панель: 5\n",
      "     - нейтраль: 1\n",
      "----------------------------------------\n",
      "3. Техническое обслуживание оборудования\n",
      "   Уверенность: 15.62%\n",
      "   Количество совпадений: 45\n",
      "   Ключевые термины:\n",
      "     - испытание: 19\n",
      "     - эксплуатация: 7\n",
      "     - тестирование: 5\n",
      "     - осмотр: 4\n",
      "     - надежность: 3\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Чтение текста из файла (DOCX, PDF, TXT).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.docx'):\n",
    "            doc = docx.Document(file_path)\n",
    "            text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \" \"\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка чтения файла: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text, language='russian'):\n",
    "    \"\"\"\n",
    "    Предобработка текста: токенизация, удаление стоп-слов и пунктуации.\n",
    "    \n",
    "    :param text: Исходный текст\n",
    "    :param language: Язык текста для определения стоп-слов\n",
    "    :return: Список слов после обработки\n",
    "    \"\"\"\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление чисел, специальных символов и знаков пунктуации\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def create_technical_categories():\n",
    "    \"\"\"\n",
    "    Создание словаря категорий с соответствующими ключевыми словами.\n",
    "    Эти категории и ключевые слова можно настроить под ваши нужды.\n",
    "    \n",
    "    :return: Словарь категорий и связанных ключевых слов\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"Электрические автоматы и выключатели\": [\n",
    "            \"автоматический выключатель\", \"расцепитель\", \"автомат\", \"выключатель\", \"прерыватель\", \n",
    "            \"masterpact\", \"compact\", \"контактор\", \"выключение\", \"защита\", \"перегрузка\", \n",
    "            \"короткое замыкание\", \"отключение\", \"автоматика\", \"электрозащита\"\n",
    "        ],\n",
    "        \n",
    "        \"Электропитание и распределение\": [\n",
    "            \"подстанция\", \"трансформатор\", \"распределение\", \"питание\", \"напряжение\", \"ток\", \n",
    "            \"сеть\", \"электроэнергия\", \"фаза\", \"нейтраль\", \"шина\", \"щит\", \"панель\", \"ввод\", \n",
    "            \"рубильник\", \"электроснабжение\", \"мощность\", \"источник питания\"\n",
    "        ],\n",
    "        \n",
    "        \"Техническое обслуживание оборудования\": [\n",
    "            \"обслуживание\", \"ремонт\", \"диагностика\", \"профилактика\", \"осмотр\", \"калибровка\", \n",
    "            \"испытание\", \"тестирование\", \"смазка\", \"очистка\", \"регулировка\", \"техобслуживание\", \n",
    "            \"запчасть\", \"износ\", \"надежность\", \"срок службы\", \"эксплуатация\"\n",
    "        ],\n",
    "        \n",
    "        \"Управление и автоматизация\": [\n",
    "            \"контроллер\", \"система\", \"автоматизация\", \"управление\", \"монитор\", \"интерфейс\", \n",
    "            \"дисплей\", \"команда\", \"параметр\", \"настройка\", \"программа\", \"алгоритм\", \"режим\", \n",
    "            \"scada\", \"датчик\", \"привод\", \"исполнительный механизм\"\n",
    "        ],\n",
    "        \n",
    "        \"Инструкция по установке\": [\n",
    "            \"установка\", \"монтаж\", \"крепление\", \"соединение\", \"подключение\", \"схема\", \"провод\", \n",
    "            \"кабель\", \"клемма\", \"зажим\", \"инструкция\", \"руководство\", \"шаг\", \"последовательность\",\n",
    "            \"инсталляция\", \"сборка\", \"демонтаж\"\n",
    "        ],\n",
    "        \n",
    "        \"Безопасность и защита\": [\n",
    "            \"безопасность\", \"защита\", \"опасность\", \"риск\", \"предупреждение\", \"авария\", \"травма\", \n",
    "            \"заземление\", \"изоляция\", \"экранирование\", \"предохранитель\", \"сигнализация\", \"тревога\", \n",
    "            \"оповещение\", \"эвакуация\", \"спасательный\", \"предупреждающий\"\n",
    "        ],\n",
    "        \n",
    "        \"Сертификация и стандарты\": [\n",
    "            \"сертификат\", \"стандарт\", \"норма\", \"требование\", \"соответствие\", \"гост\", \"регламент\", \n",
    "            \"директива\", \"iec\", \"ieee\", \"iso\", \"спецификация\", \"тестирование\", \"аттестация\", \n",
    "            \"аккредитация\", \"качество\", \"класс\"\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def classify_with_keywords(text, categories_dict, top_n=3):\n",
    "    \"\"\"\n",
    "    Классификация текста на основе ключевых слов и фраз.\n",
    "    \n",
    "    :param text: Текст для классификации\n",
    "    :param categories_dict: Словарь категорий и ключевых слов\n",
    "    :param top_n: Количество лучших категорий для вывода\n",
    "    :return: Список кортежей (категория, количество совпадений, относительный вес)\n",
    "    \"\"\"\n",
    "    # Проверка на пустой текст\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Предобработка текста\n",
    "    text_lower = text.lower()\n",
    "    processed_tokens = preprocess_text(text)\n",
    "    \n",
    "    # Подсчет встречаемости ключевых слов для каждой категории\n",
    "    results = {}\n",
    "    for category, keywords in categories_dict.items():\n",
    "        score = 0\n",
    "        keyword_counts = {}\n",
    "        \n",
    "        # Проверка вхождений каждого ключевого слова/фразы\n",
    "        for keyword in keywords:\n",
    "            # Подсчет точных фраз\n",
    "            if len(keyword.split()) > 1:\n",
    "                count = text_lower.count(keyword)\n",
    "                if count > 0:\n",
    "                    keyword_counts[keyword] = count\n",
    "                    score += count * 2  # Более высокий вес для фраз\n",
    "            # Подсчет отдельных слов\n",
    "            else:\n",
    "                # Проверяем наличие в обработанных токенах\n",
    "                count = processed_tokens.count(keyword)\n",
    "                if count > 0:\n",
    "                    keyword_counts[keyword] = count\n",
    "                    score += count\n",
    "        \n",
    "        if score > 0:\n",
    "            results[category] = {\n",
    "                'score': score,\n",
    "                'keywords': keyword_counts\n",
    "            }\n",
    "    \n",
    "    # Расчет относительных весов\n",
    "    total_score = sum(item['score'] for item in results.values())\n",
    "    \n",
    "    if total_score == 0:\n",
    "        return []\n",
    "    \n",
    "    # Формирование отсортированных результатов\n",
    "    results_list = [(category, data['score'], data['score']/total_score, data['keywords']) \n",
    "                    for category, data in results.items()]\n",
    "    results_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results_list[:top_n]\n",
    "\n",
    "def generate_report(file_path, results, output_format='text'):\n",
    "    \"\"\"\n",
    "    Генерация отчета о классификации.\n",
    "    \n",
    "    :param file_path: Путь к классифицируемому файлу\n",
    "    :param results: Результаты классификации\n",
    "    :param output_format: Формат вывода (text или json)\n",
    "    :return: Отчет в указанном формате\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return \"Классификация не удалась или не найдено совпадений с категориями.\"\n",
    "    \n",
    "    if output_format == 'json':\n",
    "        report = {\n",
    "            \"file\": os.path.basename(file_path),\n",
    "            \"classification_results\": [\n",
    "                {\n",
    "                    \"category\": category,\n",
    "                    \"score\": score,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"key_terms\": list(keywords.keys())\n",
    "                } for category, score, confidence, keywords in results\n",
    "            ]\n",
    "        }\n",
    "        return json.dumps(report, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        lines = [f\"Результаты классификации для файла: {os.path.basename(file_path)}\"]\n",
    "        lines.append(\"=\" * 80)\n",
    "        \n",
    "        for i, (category, score, confidence, keywords) in enumerate(results):\n",
    "            lines.append(f\"{i+1}. {category}\")\n",
    "            lines.append(f\"   Уверенность: {confidence:.2%}\")\n",
    "            lines.append(f\"   Количество совпадений: {score}\")\n",
    "            \n",
    "            if keywords:\n",
    "                lines.append(\"   Ключевые термины:\")\n",
    "                sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "                for term, count in sorted_keywords[:5]:  # Показываем топ-5 ключевых термина\n",
    "                    lines.append(f\"     - {term}: {count}\")\n",
    "            \n",
    "            lines.append(\"-\" * 40)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "def classify_technical_document(file_path, custom_categories=None):\n",
    "    \"\"\"\n",
    "    Полный процесс классификации технического документа.\n",
    "    \n",
    "    :param file_path: Путь к файлу\n",
    "    :param custom_categories: Пользовательские категории или None для использования предопределенных\n",
    "    :return: Результаты классификации\n",
    "    \"\"\"\n",
    "    # Чтение текста из файла\n",
    "    text = read_text_from_file(file_path)\n",
    "    if not text:\n",
    "        print(f\"Не удалось прочитать файл: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Получение категорий\n",
    "    if custom_categories is None:\n",
    "        categories = create_technical_categories()\n",
    "    else:\n",
    "        categories = custom_categories\n",
    "    \n",
    "    # Классификация\n",
    "    results = classify_with_keywords(text, categories)\n",
    "    \n",
    "    # Генерация и вывод отчета\n",
    "    report = generate_report(file_path, results)\n",
    "    print(report)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'Masterpact.pdf'  # Замените на путь к вашему файлу\n",
    "    results = classify_technical_document(file_path)\n",
    "    \n",
    "    # Если вы хотите сохранить отчет в JSON\n",
    "    # json_report = generate_report(file_path, results, output_format='json')\n",
    "    # with open('classification_report.json', 'w', encoding='utf-8') as f:\n",
    "    #     f.write(json_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44165 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты классификации для файла: Masterpact.pdf\n",
      "================================================================================\n",
      "1. Электрические автоматы и выключатели\n",
      "   Уверенность: 40.00%\n",
      "   Количество совпадений: 4\n",
      "   Ключевые термины:\n",
      "     - автоматический выключатель: 1\n",
      "     - короткое замыкание: 1\n",
      "----------------------------------------\n",
      "2. Техническое обслуживание оборудования\n",
      "   Уверенность: 40.00%\n",
      "   Количество совпадений: 4\n",
      "   Ключевые термины:\n",
      "     - срок службы: 2\n",
      "----------------------------------------\n",
      "3. Электропитание и распределение\n",
      "   Уверенность: 20.00%\n",
      "   Количество совпадений: 2\n",
      "   Ключевые термины:\n",
      "     - источник питания: 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pymorphy2\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Чтение текста из файла (DOCX, PDF, TXT).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.docx'):\n",
    "            doc = docx.Document(file_path)\n",
    "            text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \" \"\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка чтения файла: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_text(text, language='russian'):\n",
    "    \"\"\"\n",
    "    Предобработка текста с сохранением словосочетаний и лемматизацией.\n",
    "    \n",
    "    :param text: Исходный текст\n",
    "    :param language: Язык текста для определения стоп-слов\n",
    "    :return: Список слов и словосочетаний после обработки\n",
    "    \"\"\"\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление чисел, специальных символов и знаков пунктуации\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Базовая токенизация\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Лемматизация отдельных слов\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in filtered_tokens]\n",
    "    \n",
    "    # Находим биграммы и триграммы (словосочетания из 2-3 слов)\n",
    "    finder_bigrams = BigramCollocationFinder.from_words(tokens)\n",
    "    finder_trigrams = TrigramCollocationFinder.from_words(tokens)\n",
    "    \n",
    "    # Отфильтруем стоп-слова из словосочетаний\n",
    "    finder_bigrams.apply_word_filter(lambda w: w in stop_words or len(w) <= 2)\n",
    "    finder_trigrams.apply_word_filter(lambda w: w in stop_words or len(w) <= 2)\n",
    "    \n",
    "    # Находим словосочетания с высокой статистической связью\n",
    "    bigrams = finder_bigrams.nbest(BigramAssocMeasures.pmi, 50)  # top 50 биграмм\n",
    "    trigrams = finder_trigrams.nbest(TrigramAssocMeasures.pmi, 30)  # top 30 триграмм\n",
    "    \n",
    "    # Лемматизируем словосочетания\n",
    "    lemmatized_bigrams = []\n",
    "    for w1, w2 in bigrams:\n",
    "        lemma1 = morph.parse(w1)[0].normal_form\n",
    "        lemma2 = morph.parse(w2)[0].normal_form\n",
    "        lemmatized_bigrams.append(f\"{lemma1}_{lemma2}\")\n",
    "    \n",
    "    lemmatized_trigrams = []\n",
    "    for w1, w2, w3 in trigrams:\n",
    "        lemma1 = morph.parse(w1)[0].normal_form\n",
    "        lemma2 = morph.parse(w2)[0].normal_form\n",
    "        lemma3 = morph.parse(w3)[0].normal_form\n",
    "        lemmatized_trigrams.append(f\"{lemma1}_{lemma2}_{lemma3}\")\n",
    "    \n",
    "    # Добавим словосочетания к отдельным словам\n",
    "    result_tokens = lemmatized_tokens + lemmatized_bigrams + lemmatized_trigrams\n",
    "    \n",
    "    return result_tokens\n",
    "\n",
    "def create_technical_categories():\n",
    "    \"\"\"\n",
    "    Создание словаря категорий с соответствующими ключевыми словами.\n",
    "    Эти категории и ключевые слова можно настроить под ваши нужды.\n",
    "    \n",
    "    :return: Словарь категорий и связанных ключевых слов\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"Электрические автоматы и выключатели\": [\n",
    "            \"автоматический выключатель\", \"расцепитель\", \"автомат\", \"выключатель\", \"прерыватель\", \n",
    "            \"masterpact\", \"compact\", \"контактор\", \"выключение\", \"защита\", \"перегрузка\", \n",
    "            \"короткое замыкание\", \"отключение\", \"автоматика\", \"электрозащита\"\n",
    "        ],\n",
    "        \n",
    "        \"Электропитание и распределение\": [\n",
    "            \"подстанция\", \"трансформатор\", \"распределение\", \"питание\", \"напряжение\", \"ток\", \n",
    "            \"сеть\", \"электроэнергия\", \"фаза\", \"нейтраль\", \"шина\", \"щит\", \"панель\", \"ввод\", \n",
    "            \"рубильник\", \"электроснабжение\", \"мощность\", \"источник питания\"\n",
    "        ],\n",
    "        \n",
    "        \"Техническое обслуживание оборудования\": [\n",
    "            \"обслуживание\", \"ремонт\", \"диагностика\", \"профилактика\", \"осмотр\", \"калибровка\", \n",
    "            \"испытание\", \"тестирование\", \"смазка\", \"очистка\", \"регулировка\", \"техобслуживание\", \n",
    "            \"запчасть\", \"износ\", \"надежность\", \"срок службы\", \"эксплуатация\"\n",
    "        ],\n",
    "        \n",
    "        \"Управление и автоматизация\": [\n",
    "            \"контроллер\", \"система\", \"автоматизация\", \"управление\", \"монитор\", \"интерфейс\", \n",
    "            \"дисплей\", \"команда\", \"параметр\", \"настройка\", \"программа\", \"алгоритм\", \"режим\", \n",
    "            \"scada\", \"датчик\", \"привод\", \"исполнительный механизм\"\n",
    "        ],\n",
    "        \n",
    "        \"Инструкция по установке\": [\n",
    "            \"установка\", \"монтаж\", \"крепление\", \"соединение\", \"подключение\", \"схема\", \"провод\", \n",
    "            \"кабель\", \"клемма\", \"зажим\", \"инструкция\", \"руководство\", \"шаг\", \"последовательность\",\n",
    "            \"инсталляция\", \"сборка\", \"демонтаж\"\n",
    "        ],\n",
    "        \n",
    "        \"Безопасность и защита\": [\n",
    "            \"безопасность\", \"защита\", \"опасность\", \"риск\", \"предупреждение\", \"авария\", \"травма\", \n",
    "            \"заземление\", \"изоляция\", \"экранирование\", \"предохранитель\", \"сигнализация\", \"тревога\", \n",
    "            \"оповещение\", \"эвакуация\", \"спасательный\", \"предупреждающий\"\n",
    "        ],\n",
    "        \n",
    "        \"Сертификация и стандарты\": [\n",
    "            \"сертификат\", \"стандарт\", \"норма\", \"требование\", \"соответствие\", \"гост\", \"регламент\", \n",
    "            \"директива\", \"iec\", \"ieee\", \"iso\", \"спецификация\", \"тестирование\", \"аттестация\", \n",
    "            \"аккредитация\", \"качество\", \"класс\"\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def classify_with_keywords(text, categories_dict, top_n=3):\n",
    "    \"\"\"\n",
    "    Классификация текста на основе ключевых слов и фраз с учетом словосочетаний.\n",
    "    \n",
    "    :param text: Текст для классификации\n",
    "    :param categories_dict: Словарь категорий и ключевых слов\n",
    "    :param top_n: Количество лучших категорий для вывода\n",
    "    :return: Список кортежей (категория, количество совпадений, относительный вес)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Предобработка текста\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Сначала найдем все точные фразы до токенизации\n",
    "    results = {}\n",
    "    for category, keywords in categories_dict.items():\n",
    "        score = 0\n",
    "        keyword_counts = {}\n",
    "        \n",
    "        # Проверка вхождений каждого ключевого словосочетания\n",
    "        for keyword in keywords:\n",
    "            # Подсчет словосочетаний\n",
    "            if len(keyword.split()) > 1:\n",
    "                count = text_lower.count(keyword)\n",
    "                if count > 0:\n",
    "                    keyword_counts[keyword] = count\n",
    "                    score += count * 2  # Более высокий вес для фраз\n",
    "        \n",
    "        if score > 0:\n",
    "            results[category] = {\n",
    "                'score': score,\n",
    "                'keywords': keyword_counts.copy()\n",
    "            }\n",
    "        else:\n",
    "            results[category] = {\n",
    "                'score': 0,\n",
    "                'keywords': {}\n",
    "            }\n",
    "    \n",
    "    # Затем делаем предобработку с лемматизацией для отдельных слов\n",
    "    processed_tokens = preprocess_text(text)\n",
    "    \n",
    "    # Проверяем совпадения для отдельных слов и леммтизированных словосочетаний\n",
    "    for category, keywords in categories_dict.items():\n",
    "        for keyword in keywords:\n",
    "            # Только для отдельных слов\n",
    "            if len(keyword.split()) == 1:\n",
    "                # Лемматизируем ключевое слово\n",
    "                morph = pymorphy2.MorphAnalyzer()\n",
    "                lemma = morph.parse(keyword)[0].normal_form\n",
    "                \n",
    "                # Проверяем наличие в обработанных токенах\n",
    "                count = processed_tokens.count(lemma)\n",
    "                if count > 0:\n",
    "                    results[category]['keywords'][keyword] = count\n",
    "                    results[category]['score'] += count\n",
    "    \n",
    "    # Расчет относительных весов\n",
    "    total_score = sum(item['score'] for item in results.values())\n",
    "    \n",
    "    if total_score == 0:\n",
    "        return []\n",
    "    \n",
    "    # Формирование отсортированных результатов\n",
    "    results_list = [(category, data['score'], data['score']/total_score, data['keywords']) \n",
    "                    for category, data in results.items()]\n",
    "    results_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results_list[:top_n]\n",
    "\n",
    "def generate_report(file_path, results, output_format='text'):\n",
    "    \"\"\"\n",
    "    Генерация отчета о классификации.\n",
    "    \n",
    "    :param file_path: Путь к классифицируемому файлу\n",
    "    :param results: Результаты классификации\n",
    "    :param output_format: Формат вывода (text или json)\n",
    "    :return: Отчет в указанном формате\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return \"Классификация не удалась или не найдено совпадений с категориями.\"\n",
    "    \n",
    "    if output_format == 'json':\n",
    "        report = {\n",
    "            \"file\": os.path.basename(file_path),\n",
    "            \"classification_results\": [\n",
    "                {\n",
    "                    \"category\": category,\n",
    "                    \"score\": score,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"key_terms\": list(keywords.keys())\n",
    "                } for category, score, confidence, keywords in results\n",
    "            ]\n",
    "        }\n",
    "        return json.dumps(report, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        lines = [f\"Результаты классификации для файла: {os.path.basename(file_path)}\"]\n",
    "        lines.append(\"=\" * 80)\n",
    "        \n",
    "        for i, (category, score, confidence, keywords) in enumerate(results):\n",
    "            lines.append(f\"{i+1}. {category}\")\n",
    "            lines.append(f\"   Уверенность: {confidence:.2%}\")\n",
    "            lines.append(f\"   Количество совпадений: {score}\")\n",
    "            \n",
    "            if keywords:\n",
    "                lines.append(\"   Ключевые термины:\")\n",
    "                sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "                for term, count in sorted_keywords[:5]:  # Показываем топ-5 ключевых термина\n",
    "                    lines.append(f\"     - {term}: {count}\")\n",
    "            \n",
    "            lines.append(\"-\" * 40)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "def classify_technical_document(file_path, custom_categories=None):\n",
    "    \"\"\"\n",
    "    Полный процесс классификации технического документа.\n",
    "    \n",
    "    :param file_path: Путь к файлу\n",
    "    :param custom_categories: Пользовательские категории или None для использования предопределенных\n",
    "    :return: Результаты классификации\n",
    "    \"\"\"\n",
    "    # Чтение текста из файла\n",
    "    text = read_text_from_file(file_path)\n",
    "    if not text:\n",
    "        print(f\"Не удалось прочитать файл: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Получение категорий\n",
    "    if custom_categories is None:\n",
    "        categories = create_technical_categories()\n",
    "    else:\n",
    "        categories = custom_categories\n",
    "    \n",
    "    # Классификация\n",
    "    results = classify_with_keywords(text, categories)\n",
    "    \n",
    "    # Генерация и вывод отчета\n",
    "    report = generate_report(file_path, results)\n",
    "    print(report)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'Masterpact.pdf'  # Замените на путь к вашему файлу\n",
    "    results = classify_technical_document(file_path)\n",
    "    \n",
    "    # Если вы хотите сохранить отчет в JSON\n",
    "    # json_report = generate_report(file_path, results, output_format='json')\n",
    "    # with open('classification_report.json', 'w', encoding='utf-8') as f:\n",
    "    #     f.write(json_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /home/alex/miniconda3/envs/pytdml/lib/python3.10/site-packages (from ru-core-news-md==3.8.0) (2.0.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/alex/miniconda3/envs/pytdml/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (0.7.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /home/alex/miniconda3/envs/pytdml/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (2.4.417150.4580142)\n",
      "Installing collected packages: ru-core-news-md\n",
      "Successfully installed ru-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст:\n",
      "Со мной случилась такая \n",
      "вот оказия, т.е. происшествие: сегодня я гулял по \n",
      "Садовой улице и наткнулся на выхухоль.\n",
      "\n",
      "Сегментированный текст:\n",
      "1. Со мной случилась такая вот оказия, т.е. происшествие: сегодня я гулял по Садовой улице и наткнулся на выхухоль.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TextSegmenter:\n",
    "    \"\"\"Класс для сегментации текста на предложения с учетом специальных случаев\"\"\"\n",
    "    \n",
    "    def __init__(self, language: str = \"ru\", use_ml: bool = True):\n",
    "        \"\"\"\n",
    "        Инициализирует сегментатор текста\n",
    "        \n",
    "        Args:\n",
    "            language: Код языка (например, 'ru', 'en')\n",
    "            use_ml: Использовать ли ML-модели для сегментации\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        self.use_ml = use_ml\n",
    "        \n",
    "        # Загрузка языковой модели spaCy\n",
    "        if self.use_ml:\n",
    "            try:\n",
    "                if language == \"ru\":\n",
    "                    # Try to load smaller Russian model first\n",
    "                    try:\n",
    "                        self.nlp = spacy.load(\"ru_core_news_sm\")\n",
    "                    except OSError:\n",
    "                        # Fall back to medium model if small is not available\n",
    "                        self.nlp = spacy.load(\"ru_core_news_md\")\n",
    "                elif language == \"en\":\n",
    "                    # Try to load smaller English model first\n",
    "                    try:\n",
    "                        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "                    except OSError:\n",
    "                        # Fall back to medium model if small is not available\n",
    "                        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "                else:\n",
    "                    # Fallback на мультиязычную модель\n",
    "                    self.nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "            except OSError:\n",
    "                print(f\"Warning: Could not load spaCy model for {language}. Falling back to rule-based approach.\")\n",
    "                self.use_ml = False\n",
    "        \n",
    "        # Список распространенных сокращений\n",
    "        self.abbreviations = self._load_abbreviations(language)\n",
    "        \n",
    "        # Регулярные выражения для обработки переносов строк\n",
    "        self.line_break_pattern = re.compile(r'\\s*\\n\\s*')\n",
    "        \n",
    "        # Регулярные выражения для обнаружения конца предложения\n",
    "        self.sentence_end_pattern = self._create_sentence_end_pattern()\n",
    "    \n",
    "    def _load_abbreviations(self, language: str) -> List[str]:\n",
    "        \"\"\"Загружает список сокращений для заданного языка\"\"\"\n",
    "        if language == \"ru\":\n",
    "            return [\"т.е.\", \"т.д.\", \"т.п.\", \"и т.д.\", \"и т.п.\", \"др.\", \"пр.\", \"см.\", \"напр.\", \"проф.\", \"доц.\", \"акад.\"]\n",
    "        elif language == \"en\":\n",
    "            return [\"e.g.\", \"i.e.\", \"etc.\", \"vs.\", \"Mr.\", \"Mrs.\", \"Dr.\", \"Prof.\", \"Ph.D.\", \"Inc.\", \"Ltd.\"]\n",
    "        else:\n",
    "            # Базовый список сокращений\n",
    "            return [\"etc.\", \"vs.\"]\n",
    "    \n",
    "    def _create_sentence_end_pattern(self) -> re.Pattern:\n",
    "        \"\"\"Создает регулярное выражение для определения конца предложения\"\"\"\n",
    "        # Базовый шаблон для обнаружения потенциальных концов предложений\n",
    "        pattern = r'[.!?]\\s+'\n",
    "        return re.compile(pattern)\n",
    "    \n",
    "    def _is_abbreviation_ending(self, text: str, pos: int) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, является ли точка в данной позиции частью сокращения\n",
    "        \n",
    "        Args:\n",
    "            text: Текст\n",
    "            pos: Позиция точки\n",
    "            \n",
    "        Returns:\n",
    "            True, если точка - часть сокращения, иначе False\n",
    "        \"\"\"\n",
    "        # Проверка в обратном порядке от более длинных сокращений к более коротким\n",
    "        sorted_abbrs = sorted(self.abbreviations, key=len, reverse=True)\n",
    "        \n",
    "        for abbr in sorted_abbrs:\n",
    "            abbr_len = len(abbr)\n",
    "            if pos >= abbr_len - 1:  # -1 потому что abbr включает точку\n",
    "                potential_abbr = text[pos-abbr_len+1:pos+1]\n",
    "                if potential_abbr == abbr:\n",
    "                    return True\n",
    "                \n",
    "        # Проверка на инициалы типа \"А.\"\n",
    "        if pos > 0 and text[pos] == '.' and text[pos-1].isalpha() and (pos == 1 or not text[pos-2].isalpha()):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Предварительная обработка текста перед сегментацией\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            Предобработанный текст\n",
    "        \"\"\"\n",
    "        # Нормализация пробелов\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Обработка переносов строк\n",
    "        text = self._handle_line_breaks(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _handle_line_breaks(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Обрабатывает переносы строк, объединяя разорванные предложения\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            Текст с обработанными переносами строк\n",
    "        \"\"\"\n",
    "        # Заменяем одиночные переносы строк на пробелы\n",
    "        # но сохраняем двойные переносы (они обычно означают новый параграф)\n",
    "        text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "        \n",
    "        # Удаляем лишние пробелы\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def segment_text_rule_based(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Сегментирует текст на предложения с помощью правил\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            Список предложений\n",
    "        \"\"\"\n",
    "        # Предварительная обработка\n",
    "        text = self._preprocess_text(text)\n",
    "        \n",
    "        # Разделение на предложения с учетом сокращений\n",
    "        segments = []\n",
    "        start = 0\n",
    "        \n",
    "        # Находим все потенциальные концы предложений\n",
    "        for match in self.sentence_end_pattern.finditer(text):\n",
    "            end_pos = match.start()\n",
    "            \n",
    "            # Проверяем, не является ли это сокращением\n",
    "            if text[end_pos] == '.' and self._is_abbreviation_ending(text, end_pos):\n",
    "                continue\n",
    "            \n",
    "            # Это конец предложения\n",
    "            end = end_pos + 1  # Включаем знак препинания\n",
    "            segments.append(text[start:end].strip())\n",
    "            start = match.end()\n",
    "        \n",
    "        # Добавляем последний сегмент\n",
    "        if start < len(text):\n",
    "            segments.append(text[start:].strip())\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def segment_text_ml(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Сегментирует текст на предложения с помощью ML\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            Список предложений\n",
    "        \"\"\"\n",
    "        # Предварительная обработка\n",
    "        text = self._preprocess_text(text)\n",
    "        \n",
    "        # Использование spaCy для сегментации\n",
    "        doc = self.nlp(text)\n",
    "        segments = [sent.text.strip() for sent in doc.sents]\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def segment_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Сегментирует текст на предложения\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            \n",
    "        Returns:\n",
    "            Список предложений\n",
    "        \"\"\"\n",
    "        if self.use_ml:\n",
    "            return self.segment_text_ml(text)\n",
    "        else:\n",
    "            return self.segment_text_rule_based(text)\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Тестовый текст\n",
    "    test_text = \"\"\"Со мной случилась такая \n",
    "вот оказия, т.е. происшествие: сегодня я гулял по \n",
    "Садовой улице и наткнулся на выхухоль.\"\"\"\n",
    "    \n",
    "    # Создание сегментатора с автоматическим fallback на rule-based подход\n",
    "    segmenter = TextSegmenter(language=\"ru\", use_ml=True)\n",
    "    \n",
    "    # Сегментация текста\n",
    "    segments = segmenter.segment_text(test_text)\n",
    "    \n",
    "    # Вывод результатов\n",
    "    print(\"Исходный текст:\")\n",
    "    print(test_text)\n",
    "    print(\"\\nСегментированный текст:\")\n",
    "    for i, segment in enumerate(segments):\n",
    "        print(f\"{i+1}. {segment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
